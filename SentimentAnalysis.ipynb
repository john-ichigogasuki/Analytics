{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by tabularisai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john/anaconda3/envs/Analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label ID: 4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 推論関数を定義 (例)\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_id\n",
    "\n",
    "# テスト推論\n",
    "sample_text = \"I love this product. It's really amazing!\"\n",
    "sentiment = predict_sentiment(sample_text)\n",
    "print(\"Predicted label ID:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Very Negative', 1: 'Negative', 2: 'Neutral', 3: 'Positive', 4: 'Very Positive'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Koga.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Koga.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Oyama.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Oyama.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Koga.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Koga.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Oyama.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Oyama.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filepath: /path/to/SentimentAnalysis_tabularisai.py\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_sentiment_in_csvs(input_pattern: str):\n",
    "    \"\"\"\n",
    "    指定したパターンにマッチするCSVファイルをすべて読み込み、\n",
    "    'Detail'列 (index=1) に対して感情分析を行い、\n",
    "    新たな列 'Sentiment-tabularisai' (ラベル) と 'Sentiment-tabularisai-score' (スコア) を\n",
    "   追加して、同じファイル名で上書き保存する関数。\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_pattern : str\n",
    "        読み込むファイルパスのパターン。\n",
    "        例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Sentiment Analysis パイプラインの準備\n",
    "    #    多言語の感情分析を想定し、Hugging Faceから公開されているモデルを利用\n",
    "    #    例: \"tabularisai/multilingual-sentiment-analysis\"\n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"sentiment-analysis\",\n",
    "        model=\"tabularisai/multilingual-sentiment-analysis\"\n",
    "    )\n",
    "\n",
    "    # 2. 指定パターンに合うCSVファイルをすべて取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"パターン {input_pattern} に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中: {csv_path} ===\")\n",
    "\n",
    "        # 3. CSVを読み込む\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # dfのカラム構造:\n",
    "        #   column[0]: Speaker\n",
    "        #   column[1]: Detail\n",
    "        #   column[2]: UniqueWords\n",
    "        #   column[3]: JLPTLevel\n",
    "        # → 新たに column[4]: Sentiment-tabularisai\n",
    "        #               column[5]: Sentiment-tabularisai-score を追加\n",
    "\n",
    "        # 4. Detail列に対して感情分析を適用\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            result = sentiment_pipeline(text)\n",
    "            label = result[0][\"label\"]   # 例: \"positive\", \"negative\", \"neutral\"など\n",
    "            score = result[0][\"score\"]   # 0.0～1.0の範囲\n",
    "\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        # 5. 新たな列として追加\n",
    "        df[\"Sentiment-tabularisai\"] = labels\n",
    "        df[\"Sentiment-tabularisai-score\"] = scores\n",
    "\n",
    "        # 6. 同じファイル名で上書き保存\n",
    "        #    ※ 元データが必要な場合はバックアップを取るなどご注意ください\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\" で\n",
    "    # analyzed_alpha_{name}.csv, analyzed_beta_{name}.csv を処理する想定。\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    analyze_sentiment_in_csvs(input_pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = './Analyzed/AnalyzedData/analyzed_*_*.csv'\n",
    "def sentimentAnalyzeByTabularisai(file_path):\n",
    "    file = pd.read_csv(file)\n",
    "    df = pd.DataFrame(file)\n",
    "    df = df.drop(columns=['Sentiment'])\n",
    "    df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ファイルパスのパターンを指定\n",
    "file_pattern = './Analyzed/AnalyzedData/analyzed_*_*.csv'\n",
    "\n",
    "# ファイルパスのリストを取得\n",
    "file_paths = glob.glob(file_pattern)\n",
    "\n",
    "def sentimentAnalyzeByTabularisai(file_path):\n",
    "    # CSVファイルを読み込む\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 'Sentiment'列を削除する\n",
    "    if 'Sentiment' in df.columns:\n",
    "        df = df.drop(columns=['Sentiment'])\n",
    "    \n",
    "    # 元のファイルに上書き保存\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# すべてのファイルに対して処理を実行\n",
    "for file_path in file_paths:\n",
    "    sentimentAnalyzeByTabularisai(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by lxyuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filepath: /path/to/SentimentAnalysis_lxyuan.py\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_with_lxyuan(input_pattern: str):\n",
    "    \"\"\"\n",
    "    指定したパターンにマッチするCSVファイルをすべて読み込み、\n",
    "    'Detail'列に対してlxyuanモデルで感情分析を行い、\n",
    "    新たな列 'Sentiment-lxyuan' と 'Sentiment-lxyuan-score' を追加して上書き保存する関数。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_pattern : str\n",
    "        読み込むファイルパスのパターン (例: \"./Analyzed/AnalyzedData/analyzed_*_*.csv\")\n",
    "    model_name : str\n",
    "        使用するlxyuanモデル名。デフォルトは \"lxyuan\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 感情分析パイプラインの準備\n",
    "    #    \"text-classification\"タスクとして、lxyuanのモデルを使用\n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "    )\n",
    "\n",
    "    # 2. 指定パターンに合うCSVファイルを取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"指定のパターン '{input_pattern}' に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    # 3. ファイルを順次読み込んで処理\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中ファイル: {csv_path} ===\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 4. \"Detail\"列に対して感情分析を実施\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            result = sentiment_pipeline(text)\n",
    "            # resultは [{'label': <str>, 'score': <float>}]\n",
    "            label = result[0][\"label\"]   # ラベル名 (例: \"POSITIVE\", \"NEGATIVE\" など)\n",
    "            score = result[0][\"score\"]   # 0.0 ～ 1.0 の範囲\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        # 5. 新たな列として結果を追加\n",
    "        df[\"Sentiment-lxyuan\"] = labels\n",
    "        df[\"Sentiment-lxyuan-score\"] = scores\n",
    "\n",
    "        # 6. 上書き保存 (バックアップが必要であれば別途コピーを推奨)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # フォルダ内のCSVを一括処理するパターンを指定\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    # 関数を呼び出して処理を実行\n",
    "    analyze_with_lxyuan(input_pattern=pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jarvisx17/japanese-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filepath: /path/to/SentimentAnalysis_jarvisx17.py\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_with_jarvisx17(input_pattern: str):\n",
    "\n",
    "    # 1. 感情分析パイプラインの準備\n",
    "    #    \"text-classification\"タスクとして、jarvisx17のモデルを使用\n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=\"jarvisx17/japanese-sentiment-analysis\"\n",
    "    )\n",
    "\n",
    "    # 2. 指定パターンに合うCSVファイルを取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"指定のパターン '{input_pattern}' に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    # 3. ファイルを順次読み込んで処理\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中ファイル: {csv_path} ===\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 4. \"Detail\"列に対して感情分析を実施\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            result = sentiment_pipeline(text)\n",
    "            # resultは [{'label': <str>, 'score': <float>}]\n",
    "            label = result[0][\"label\"]   # ラベル名 (例: \"POSITIVE\", \"NEGATIVE\" など)\n",
    "            score = result[0][\"score\"]   # 0.0 ～ 1.0 の範囲\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        # 5. 新たな列として結果を追加\n",
    "        df[\"Sentiment-jarvisx17\"] = labels\n",
    "        df[\"Sentiment-jarvisx17-score\"] = scores\n",
    "\n",
    "        # 6. 上書き保存 (バックアップが必要であれば別途コピーを推奨)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # フォルダ内のCSVを一括処理するパターンを指定\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    # 関数を呼び出して処理を実行\n",
    "    analyze_with_jarvisx17(input_pattern=pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis by Mizuho-Sakura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "=== 処理中ファイル: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filepath: /path/to/SentimentAnalysis_MizuhoSakura.py\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "def analyze_with_MizuhoSakura(input_pattern: str):\n",
    "\n",
    "    # 1. 感情分析パイプラインの準備\n",
    "    #    \"text-classification\"タスクとして、MizuhoSakuraのモデルを使用\n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=\"Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime\"\n",
    "    )\n",
    "\n",
    "    # 2. 指定パターンに合うCSVファイルを取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"指定のパターン '{input_pattern}' に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    # 3. ファイルを順次読み込んで処理\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中ファイル: {csv_path} ===\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 4. \"Detail\"列に対して感情分析を実施\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            result = sentiment_pipeline(text)\n",
    "            # resultは [{'label': <str>, 'score': <float>}]\n",
    "            label = result[0][\"label\"]   # ラベル名 (例: \"POSITIVE\", \"NEGATIVE\" など)\n",
    "            score = result[0][\"score\"]   # 0.0 ～ 1.0 の範囲\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        # 5. 新たな列として結果を追加\n",
    "        df[\"Sentiment-MizuhoSakura\"] = labels\n",
    "        df[\"Sentiment-MizuhoSakura-score\"] = scores\n",
    "\n",
    "        # 6. 上書き保存 (バックアップが必要であれば別途コピーを推奨)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # フォルダ内のCSVを一括処理するパターンを指定\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    # 関数を呼び出して処理を実行\n",
    "    analyze_with_MizuhoSakura(input_pattern=pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cristian phu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = \"christian-phu/bert-finetuned-japanese-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "local_model_dir = \"./Models/\"\n",
    "\n",
    "model.save_pretrained(local_model_dir)\n",
    "tokenizer.save_pretrained(local_model_dir)\n",
    "\n",
    "def analyze_with_Christian(input_pattern: str, local_model_dir):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(local_model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "    \n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    for csv_path in csv_files:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            result = sentiment_pipeline(text)\n",
    "            label = result[0][\"label\"]\n",
    "            score = result[0][\"score\"]\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "        \n",
    "        df[\"Sentiment-Christian\"] = labels\n",
    "        df[\"Sentiment-Christian-score\"] = scores\n",
    "        \n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    local_model_dir = \"./Models/\"\n",
    "    analyze_with_Christian(input_pattern=pattern, local_model_dir=local_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Detail</th>\n",
       "      <th>WordsLen</th>\n",
       "      <th>UniqueWords</th>\n",
       "      <th>JLPTLevel</th>\n",
       "      <th>Sentiment-tabularisai</th>\n",
       "      <th>Sentiment-tabularisai-score</th>\n",
       "      <th>Sentiment-lxyuan</th>\n",
       "      <th>Sentiment-lxyuan-score</th>\n",
       "      <th>Sentiment-Christian</th>\n",
       "      <th>Sentiment-Christian-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI</td>\n",
       "      <td>こんにちは！最近、体の調子はどうですか？何か運動やリラクゼーションの活動をしていますか？それ...</td>\n",
       "      <td>74</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Very Negative</td>\n",
       "      <td>0.398403</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.401744</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.998923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User</td>\n",
       "      <td>最近は起きた時に寝ても寝ても眠い感じがする</td>\n",
       "      <td>21</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.461021</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.896546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI</td>\n",
       "      <td>それはちょっと大変そうですね。睡眠の質が落ちている可能性があるかもしれませんね。寝る前の環境...</td>\n",
       "      <td>138</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Very Negative</td>\n",
       "      <td>0.561873</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User</td>\n",
       "      <td>スマホを見てしまうことなどは前からあったけど，前よりも疲れている感じがするんだよね</td>\n",
       "      <td>41</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.469847</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.773605</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.969296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>疲れが溜まっていると感じるときは、やっぱり体も心も重くなりますよね。スマホの光が睡眠の質を下...</td>\n",
       "      <td>274</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.339138</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.517148</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.800267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>User</td>\n",
       "      <td>散歩するのに家の周りが面白くないんだけど，散歩が面白くなるアイデアが欲しいんだよな</td>\n",
       "      <td>41</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.397680</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.456570</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.965423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AI</td>\n",
       "      <td>散歩をもっと楽しくするアイデア、いくつか提案しますね！ポッドキャストやオーディオブックを聴く...</td>\n",
       "      <td>449</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.645495</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.766437</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.998347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AI Average</td>\n",
       "      <td>Mean Unique Words</td>\n",
       "      <td>17</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.424112</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.994090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>User Average</td>\n",
       "      <td>Mean Unique Words</td>\n",
       "      <td>17</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.424112</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.994090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Speaker                                             Detail  WordsLen  \\\n",
       "0            AI  こんにちは！最近、体の調子はどうですか？何か運動やリラクゼーションの活動をしていますか？それ...        74   \n",
       "1          User                              最近は起きた時に寝ても寝ても眠い感じがする        21   \n",
       "2            AI  それはちょっと大変そうですね。睡眠の質が落ちている可能性があるかもしれませんね。寝る前の環境...       138   \n",
       "3          User          スマホを見てしまうことなどは前からあったけど，前よりも疲れている感じがするんだよね        41   \n",
       "4            AI  疲れが溜まっていると感じるときは、やっぱり体も心も重くなりますよね。スマホの光が睡眠の質を下...       274   \n",
       "5          User          散歩するのに家の周りが面白くないんだけど，散歩が面白くなるアイデアが欲しいんだよな        41   \n",
       "6            AI  散歩をもっと楽しくするアイデア、いくつか提案しますね！ポッドキャストやオーディオブックを聴く...       449   \n",
       "7    AI Average                                  Mean Unique Words        17   \n",
       "8  User Average                                  Mean Unique Words        17   \n",
       "\n",
       "   UniqueWords  JLPTLevel Sentiment-tabularisai  Sentiment-tabularisai-score  \\\n",
       "0    36.000000        3.0         Very Negative                     0.398403   \n",
       "1    13.000000        2.0              Negative                     0.338300   \n",
       "2    57.000000        2.0         Very Negative                     0.561873   \n",
       "3    25.000000        2.0               Neutral                     0.469847   \n",
       "4    90.000000        2.0               Neutral                     0.339138   \n",
       "5    18.000000        2.0               Neutral                     0.397680   \n",
       "6   126.000000        3.0              Positive                     0.645495   \n",
       "7    77.250000        3.0               Neutral                     0.424112   \n",
       "8    18.666667        3.0               Neutral                     0.424112   \n",
       "\n",
       "  Sentiment-lxyuan  Sentiment-lxyuan-score Sentiment-Christian  \\\n",
       "0         positive                0.401744            positive   \n",
       "1         negative                0.461021            positive   \n",
       "2         negative                0.730563             neutral   \n",
       "3         negative                0.773605             neutral   \n",
       "4         positive                0.517148             neutral   \n",
       "5         positive                0.456570             neutral   \n",
       "6         positive                0.766437            positive   \n",
       "7         positive                0.511583            positive   \n",
       "8         positive                0.511583            positive   \n",
       "\n",
       "   Sentiment-Christian-score  \n",
       "0                   0.998923  \n",
       "1                   0.896546  \n",
       "2                   0.999262  \n",
       "3                   0.969296  \n",
       "4                   0.800267  \n",
       "5                   0.965423  \n",
       "6                   0.998347  \n",
       "7                   0.994090  \n",
       "8                   0.994090  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('./Analyzed/AnalyzedData/analyzed_alpha_abe.csv')\n",
    "df = pd.DataFrame(file)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### このファイルで行った感情分析Columnをリセットする場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "上書き保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def remove_columns_in_csvs(input_pattern: str):\n",
    "    \"\"\"\n",
    "    指定したパターンにマッチするCSVファイルをすべて読み込み、\n",
    "    指定した列（column 4以降）を削除して、同じファイル名で上書き保存する関数。\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_pattern : str\n",
    "        読み込むファイルパスのパターン。\n",
    "        例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 指定パターンに合うCSVファイルをすべて取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"パターン {input_pattern} に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中: {csv_path} ===\")\n",
    "\n",
    "        # CSVを読み込む\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 指定した列（column 4以降）を削除\n",
    "        df = df.iloc[:, :8]\n",
    "\n",
    "        # 同じファイル名で上書き保存\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"上書き保存完了: {csv_path}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\" で\n",
    "    # analyzed_alpha_{name}.csv, analyzed_beta_{name}.csv を処理する想定。\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    remove_columns_in_csvs(input_pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
