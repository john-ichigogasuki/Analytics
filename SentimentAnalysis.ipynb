{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john/anaconda3/envs/newenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label ID: 4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 推論関数を定義 (例)\n",
    "def predict_sentiment(text: str):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_id\n",
    "\n",
    "# テスト推論\n",
    "sample_text = \"I love this product. It's really amazing!\"\n",
    "sentiment = predict_sentiment(sample_text)\n",
    "print(\"Predicted label ID:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Very Negative', 1: 'Negative', 2: 'Neutral', 3: 'Positive', 4: 'Very Positive'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Hiraoka_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Mori.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Mori_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Abe.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Abe_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Suto.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Suto_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Sakai.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Sakai_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Pang.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Pang_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Futamura.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Futamura_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Mori.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Mori_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Futamura_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Sakai_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Shiojiri_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Suto.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Suto_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_beta_Hiraoka_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Abe.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Abe_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Shiojiri_sentiment.csv\n",
      "\n",
      "=== 処理中: ./Analyzed/AnalyzedData/analyzed_alpha_Pang.csv ===\n",
      "保存完了: ./Analyzed/AnalyzedData/analyzed_alpha_Pang_sentiment.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def analyze_sentiment_in_csvs(input_pattern: str, output_suffix: str = \"_sentiment\"):\n",
    "    \"\"\"\n",
    "    指定したパターンにマッチするCSVファイルをすべて読み込み、\n",
    "    'Detail' 列 (index=1) に対して感情分析を行い、\n",
    "    新たな列 (Sentiment) を追加して別名で保存する関数。\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_pattern : str\n",
    "        読み込むファイルパスのパターン。例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    output_suffix : str\n",
    "        保存時に付与するサフィックス。デフォルトは \"_sentiment\"。\n",
    "        例: analyzed_alpha_example.csv → analyzed_alpha_example_sentiment.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Sentiment Analysis パイプラインの準備\n",
    "    #    多言語の感情分析を想定し、Hugging Faceから公開されているモデルを利用\n",
    "    #    ここでは例として \"tabularisai/multilingual-sentiment-analysis\" を使用\n",
    "    sentiment_pipeline = pipeline(\n",
    "        task=\"sentiment-analysis\",\n",
    "        model=\"tabularisai/multilingual-sentiment-analysis\"\n",
    "    )\n",
    "\n",
    "    # 2. 指定パターンに合うCSVをすべて取得\n",
    "    csv_files = glob.glob(input_pattern)\n",
    "    if not csv_files:\n",
    "        print(f\"パターン {input_pattern} に合致するCSVファイルがありません。処理を終了します。\")\n",
    "        return\n",
    "\n",
    "    for csv_path in csv_files:\n",
    "        print(f\"=== 処理中: {csv_path} ===\")\n",
    "\n",
    "        # 3. CSVを読み込む\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # df のカラム構造: \n",
    "        #   column[0]: Speaker\n",
    "        #   column[1]: Detail\n",
    "        #   column[2]: UniqueWords\n",
    "        #   column[3]: JLPTLevel\n",
    "        # → 新たに column[4] (== 5列目) に感情分析結果を追加したい\n",
    "        # ここでは \"Sentiment\" という列名にする\n",
    "\n",
    "        # 4. Detail列に対して感情分析を適用\n",
    "        #    1行ずつパイプラインに渡す → 感情ラベルを取得\n",
    "        #    例: \"positive\", \"negative\", \"neutral\" など\n",
    "        sentiments = []\n",
    "        for text in df[\"Detail\"]:\n",
    "            # パイプラインに text を与えると list[dict] が返る\n",
    "            # [ {'label': 'positive', 'score': 0.999...}, ... ]\n",
    "            # ここでは label だけを取得\n",
    "            result = sentiment_pipeline(text)\n",
    "            label = result[0][\"label\"]\n",
    "            sentiments.append(label)\n",
    "\n",
    "        # 5. 新しい列として追加\n",
    "        df[\"Sentiment\"] = sentiments  # ここで列の位置は df の末尾 (通常は5列目) になります\n",
    "\n",
    "        # 6. 結果のCSVを保存\n",
    "        #    例: \"analyzed_alpha_example.csv\" → \"analyzed_alpha_example_sentiment.csv\"\n",
    "        base_name = os.path.basename(csv_path)  # ファイル名のみ取得\n",
    "        name_part, ext = os.path.splitext(base_name)\n",
    "        output_name = f\"{name_part}{output_suffix}{ext}\"  # suffixを付けて保存ファイル名を生成\n",
    "\n",
    "        output_path = os.path.join(os.path.dirname(csv_path), output_name)\n",
    "        df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"保存完了: {output_path}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ここで実行パラメータを設定して関数を呼び出す\n",
    "    # 例: \"root/Analyzed/AnalyzedData/analyzed_*_*.csv\" で \n",
    "    #     analyzed_alpha_{name}.csv, analyzed_beta_{name}.csv を一括処理する想定。\n",
    "    pattern = \"./Analyzed/AnalyzedData/analyzed_*_*.csv\"\n",
    "    \n",
    "    analyze_sentiment_in_csvs(input_pattern=pattern, output_suffix=\"_sentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
