{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abe のデータを読み込みました。\n",
      "形態素解析を適用しました。\n",
      "頻出単語を集計しました。\n",
      "結果を ./Analyzed/MorphologicalAnalysis/morphorogicalcalAnalyzed_alpha_Abe_frequent_words.csv に保存しました。\n",
      "結果を ./Analyzed/MorphologicalAnalysis/morphologicalAnalyzed_beta_Abe_frequent_words.csv に保存しました。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from collections import Counter\n",
    "\n",
    "# ユーザーに名前を入力させる\n",
    "name = input(\"対象者の名前を入力してください: \")\n",
    "\n",
    "# ファイルのパス設定\n",
    "input_alpha_path = f\"./TalkData/alpha_{name}.csv\"\n",
    "input_beta_path = f\"./TalkData/beta_{name}.csv\"\n",
    "\n",
    "output_alpha_path = f\"./Analyzed/MorphologicalAnalysis/morphorogicalcalAnalyzed_alpha_{name}_frequent_words.csv\"\n",
    "output_beta_path = f\"./Analyzed/MorphologicalAnalysis/morphologicalAnalyzed_beta_{name}_frequent_words.csv\"\n",
    "\n",
    "# データの読み込み\n",
    "if not os.path.exists(input_alpha_path) or not os.path.exists(input_beta_path):\n",
    "    raise FileNotFoundError(\"指定された名前のデータが見つかりません。\")\n",
    "\n",
    "alpha_df = pd.read_csv(input_alpha_path)\n",
    "beta_df = pd.read_csv(input_beta_path)\n",
    "\n",
    "print(f\"{name} のデータを読み込みました。\")\n",
    "\n",
    "# MeCabの初期化\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# 形態素解析を適用する関数\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return tagger.parse(text).strip()\n",
    "\n",
    "# データに形態素解析を適用\n",
    "alpha_df[\"Tokenized\"] = alpha_df[\"Detail\"].apply(tokenize_text)\n",
    "beta_df[\"Tokenized\"] = beta_df[\"Detail\"].apply(tokenize_text)\n",
    "\n",
    "print(\"形態素解析を適用しました。\")\n",
    "\n",
    "# ストップワード（不要な単語のリスト）\n",
    "stopwords = set([\"は\", \"が\", \"の\", \"です\", \"ます\", \"を\", \"に\", \"で\", \"と\", \"も\", \"ない\", \"し\", \"た\", \"な\", \"その\"])\n",
    "\n",
    "# 頻出単語を集計する関数\n",
    "def count_frequent_words(df):\n",
    "    all_words = []\n",
    "    for text in df[\"Tokenized\"]:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        all_words.extend(words)\n",
    "\n",
    "    return Counter(all_words)\n",
    "\n",
    "# 頻出単語を取得\n",
    "alpha_word_counts = count_frequent_words(alpha_df)\n",
    "beta_word_counts = count_frequent_words(beta_df)\n",
    "\n",
    "print(\"頻出単語を集計しました。\")\n",
    "\n",
    "# 頻出単語の上位50単語をデータフレームに変換\n",
    "alpha_word_df = pd.DataFrame(alpha_word_counts.most_common(50), columns=[\"Word\", \"Count\"])\n",
    "beta_word_df = pd.DataFrame(beta_word_counts.most_common(50), columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# 結果を保存\n",
    "alpha_word_df.to_csv(output_alpha_path, index=False, encoding=\"utf-8-sig\")\n",
    "beta_word_df.to_csv(output_beta_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"結果を {output_alpha_path} に保存しました。\")\n",
    "print(f\"結果を {output_beta_path} に保存しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abe のデータを読み込みました。\n",
      "形態素解析を適用しました。\n",
      "頻出単語を集計しました。\n",
      "結果を ./Analyzed/MorphologicalAnalysis/morphorogicalcalAnalyzed_alpha_Abe_frequent_words.csv に保存しました。\n",
      "結果を ./Analyzed/MorphologicalAnalysis/morphologicalAnalyzed_beta_Abe_frequent_words.csv に保存しました。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from collections import Counter\n",
    "\n",
    "# ユーザーに名前を入力させる\n",
    "name = input(\"対象者の名前を入力してください: \")\n",
    "\n",
    "# ファイルのパス設定\n",
    "input_alpha_path = f\"./TalkData/alpha_{name}.csv\"\n",
    "input_beta_path = f\"./TalkData/beta_{name}.csv\"\n",
    "\n",
    "output_alpha_path = f\"./Analyzed/MorphologicalAnalysis/morphorogicalcalAnalyzed_alpha_{name}_frequent_words.csv\"\n",
    "output_beta_path = f\"./Analyzed/MorphologicalAnalysis/morphologicalAnalyzed_beta_{name}_frequent_words.csv\"\n",
    "\n",
    "# データの読み込み\n",
    "if not os.path.exists(input_alpha_path) or not os.path.exists(input_beta_path):\n",
    "    raise FileNotFoundError(\"指定された名前のデータが見つかりません。\")\n",
    "\n",
    "alpha_df = pd.read_csv(input_alpha_path)\n",
    "beta_df = pd.read_csv(input_beta_path)\n",
    "\n",
    "print(f\"{name} のデータを読み込みました。\")\n",
    "\n",
    "# MeCabの初期化\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# 形態素解析を適用する関数\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return tagger.parse(text).strip()\n",
    "\n",
    "# データに形態素解析を適用\n",
    "alpha_df[\"Tokenized\"] = alpha_df[\"Detail\"].apply(tokenize_text)\n",
    "beta_df[\"Tokenized\"] = beta_df[\"Detail\"].apply(tokenize_text)\n",
    "\n",
    "print(\"形態素解析を適用しました。\")\n",
    "\n",
    "# ストップワード（不要な単語のリスト）\n",
    "stopwords = set([\"は\", \"が\", \"の\", \"です\", \"ます\", \"を\", \"に\", \"で\", \"と\", \"も\", \"ない\", \"し\", \"た\", \"な\", \"その\"])\n",
    "\n",
    "# 頻出単語を集計する関数\n",
    "def count_frequent_words(df):\n",
    "    all_words = []\n",
    "    for text in df[\"Tokenized\"]:\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in stopwords]\n",
    "        all_words.extend(words)\n",
    "\n",
    "    return Counter(all_words)\n",
    "\n",
    "# 頻出単語を取得\n",
    "alpha_word_counts = count_frequent_words(alpha_df)\n",
    "beta_word_counts = count_frequent_words(beta_df)\n",
    "\n",
    "print(\"頻出単語を集計しました。\")\n",
    "\n",
    "# 頻出単語の上位50単語をデータフレームに変換\n",
    "alpha_word_df = pd.DataFrame(alpha_word_counts.most_common(50), columns=[\"Word\", \"Count\"])\n",
    "beta_word_df = pd.DataFrame(beta_word_counts.most_common(50), columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# 結果を保存\n",
    "alpha_word_df.to_csv(output_alpha_path, index=False, encoding=\"utf-8-sig\")\n",
    "beta_word_df.to_csv(output_beta_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"結果を {output_alpha_path} に保存しました。\")\n",
    "print(f\"結果を {output_beta_path} に保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.strings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMeCab\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ユーザーに名前を入力させる\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/sklearn/utils/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/scipy/sparse/__init__.py:294\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/scipy/sparse/_base.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[1;32m      8\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[1;32m      9\u001b[0m                        matrix, validateaxis,)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/scipy/_lib/_util.py:18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Optional,\n\u001b[1;32m     12\u001b[0m     Union,\n\u001b[1;32m     13\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     14\u001b[0m     TypeVar,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[1;32m     21\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/scipy/_lib/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     is_array_api_obj,\n\u001b[1;32m     19\u001b[0m     size,\n\u001b[1;32m     20\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/study/lib/python3.10/site-packages/numpy/__init__.py:379\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_mac_os_check\u001b[39m():\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;124;03m    Quick Sanity check for Mac OS look for accelerate build bugs.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    Testing numpy polyfit calls init_dgelsd(LAPACK)\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m         c \u001b[38;5;241m=\u001b[39m array([\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m1.\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "\n",
    "# ユーザーに名前を入力させる\n",
    "name = input(\"対象者の名前を入力してください: \")\n",
    "\n",
    "# ファイルのパス設定\n",
    "input_alpha_path = f\"./TalkData/alpha_{name}.csv\"\n",
    "input_beta_path = f\"./TalkData/beta_{name}.csv\"\n",
    "\n",
    "output_alpha_path = f\"./Analyzed/LDA/lda_alpha_{name}.csv\"\n",
    "output_beta_path = f\"./Analyzed/LDA/lda_beta_{name}.csv\"\n",
    "\n",
    "# データの読み込み\n",
    "if not os.path.exists(input_alpha_path) or not os.path.exists(input_beta_path):\n",
    "    raise FileNotFoundError(\"指定された名前のデータが見つかりません。\")\n",
    "\n",
    "alpha_df = pd.read_csv(input_alpha_path)\n",
    "beta_df = pd.read_csv(input_beta_path)\n",
    "\n",
    "print(f\"{name} のデータを読み込みました。\")\n",
    "\n",
    "# MeCabの初期化\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "# 形態素解析を適用する関数\n",
    "def tokenize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return tagger.parse(text).strip()\n",
    "\n",
    "# データに形態素解析を適用\n",
    "alpha_df[\"Tokenized\"] = alpha_df[\"Detail\"].apply(tokenize_text)\n",
    "beta_df[\"Tokenized\"] = beta_df[\"Detail\"].apply(tokenize_text)\n",
    "\n",
    "print(\"形態素解析を適用しました。\")\n",
    "\n",
    "# トピック数を設定\n",
    "num_topics = 3\n",
    "\n",
    "# トピックモデリングを実行する関数\n",
    "def perform_lda(df, num_topics):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df[\"Tokenized\"])\n",
    "\n",
    "    # 単語リストを取得\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # LDAの実行\n",
    "    lda_model = models.LdaModel(corpus=[[(i, int(X[j, i])) for i in range(X.shape[1])] for j in range(X.shape[0])],\n",
    "                                id2word=dict(enumerate(words)),\n",
    "                                num_topics=num_topics,\n",
    "                                random_state=42)\n",
    "\n",
    "    return lda_model, words\n",
    "\n",
    "# LDAを適用\n",
    "alpha_lda, alpha_words = perform_lda(alpha_df, num_topics)\n",
    "beta_lda, beta_words = perform_lda(beta_df, num_topics)\n",
    "\n",
    "print(\"トピックモデリングを実行しました。\")\n",
    "\n",
    "# トピックごとの単語を取得\n",
    "def get_topic_words(lda_model, words, num_words=10):\n",
    "    topics = []\n",
    "    for topic_id in range(num_topics):\n",
    "        topic_terms = lda_model.get_topic_terms(topic_id, num_words)\n",
    "        topic_words = [words[word_id] for word_id, _ in topic_terms]\n",
    "        topics.append(\", \".join(topic_words))\n",
    "    return topics\n",
    "\n",
    "alpha_topics = get_topic_words(alpha_lda, alpha_words)\n",
    "beta_topics = get_topic_words(beta_lda, beta_words)\n",
    "\n",
    "# トピックの結果を保存\n",
    "alpha_topic_df = pd.DataFrame({\"Topic\": range(1, num_topics + 1), \"Top Words\": alpha_topics})\n",
    "beta_topic_df = pd.DataFrame({\"Topic\": range(1, num_topics + 1), \"Top Words\": beta_topics})\n",
    "\n",
    "alpha_topic_df.to_csv(output_alpha_path, index=False, encoding=\"utf-8-sig\")\n",
    "beta_topic_df.to_csv(output_beta_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"結果を {output_alpha_path} に保存しました。\")\n",
    "print(f\"結果を {output_beta_path} に保存しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
